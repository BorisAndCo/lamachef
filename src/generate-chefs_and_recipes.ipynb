{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *La Main à la Pâte*, Generate Chefs and Recipes\n",
    "*--- WIP ---*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from ollama import Client\n",
    "from pydantic import BaseModel\n",
    "from devtools import pprint\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.debug(\"\")\n",
    "\n",
    "from datetime import datetime\n",
    "import helper_io as helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='C:\\\\Users\\\\antgr\\\\anaconda3\\\\envs\\\\web310\\\\Library\\\\ssl\\\\cacert.pem'\n"
     ]
    }
   ],
   "source": [
    "# Conf. ollama and select model\n",
    "HOST_LLAMA=os.environ['HOST_LLAMA']\n",
    "LLAMA_MODEL=\"llama3.2\"\n",
    "\n",
    "client = Client(\n",
    "  host=HOST_LLAMA,\n",
    "  headers={'x-some-header': 'some-value'}\n",
    ")\n",
    "\n",
    "# Conf. io directories\n",
    "WORKDIR = Path(os.getcwd())\n",
    "DIR_SCHEMAS = Path(WORKDIR.parent, 'json_schemas')\n",
    "DIR_OUTPUT = Path(WORKDIR.parent, 'out')\n",
    "DIR_PROMPTS = Path(WORKDIR.parent, 'prompts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "To use Ollama's structured output, the LLM-model answer to the prompt is constrained with a *JSON Schema*, conveniently registered in Python with a Pydantic Model.\n",
    "The *JSON schemas* are registered in the `DIR_MODELS` directory. We provide ios functions for saving/loading schemas in `src/helper_io.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemples of models\n",
    "class Chef(BaseModel):\n",
    "    familyName:     str\n",
    "    givenName:      str\n",
    "    alternateName:  str\n",
    "    birthDate:      str # Problem with pydantic datetime declaration\n",
    "    gender:         str\n",
    "    description:    str\n",
    "    homeLocation:   str \n",
    "    image:          str \n",
    "    prefCuisine:    list[str] \n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    id_chef_creator:        str\n",
    "    name:                   str\n",
    "    recipeIngredients:      list[str]\n",
    "    recipeYield:            str\n",
    "    recipeInstructions:     list[str]\n",
    "    prepTime:               int\n",
    "    cookTime:               int\n",
    "    description:            str\n",
    "    image:                  str\n",
    "    recipeCategory:         str\n",
    "    recipeCuisine:          list[str]\n",
    "    keywords:               list[str]\n",
    "\n",
    "\n",
    "# # Save an existing model\n",
    "# helper.save_pydantic_model_as_jsonschema(directory_schema=DIR_SCHEMAS, schema_pydmodel=Chef)\n",
    "# helper.save_pydantic_model_as_jsonschema(directory_schema=DIR_SCHEMAS, schema_pydmodel=Recipe)\n",
    "\n",
    "# Load an existing model\n",
    "# Animal = helper.load_jsonschema_to_pydantic_model(json.loads(Path(WORKDIR.parent, 'json_schemas', 'Animal.json').read_text()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts\n",
    "*--- WIP ---*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:helper_io:Saving prompt Chef-b102 to c:\\_Etudes\\25_Main_Patte\\lamachef\\prompts\\Chef-b102.md\n"
     ]
    }
   ],
   "source": [
    "# Save a prompt\n",
    "# helper.save_pydantic_model(directory_models: str|Path, model: BaseModel)\n",
    "\n",
    "# Load an existing prompt\n",
    "# helper.load_pydantic_model(directory_models: str|Path, model: BaseModel) -> BaseModel\n",
    "\n",
    "current_chefschema=Chef\n",
    "current_chefprompt_name=\"Chef-b102\"\n",
    "current_chefprompt = \"\"\"You are the editor-in-chief of a famous international restaurant and cooking guide, open to all audiences, from the most modest to the most gastronomic (for example traditional french cuisine, chinese street food or english bistronomy).\n",
    "You want to exhibit one of you favorite chief in the pages of your next issue. As a summary, you specify some key characteristics of this chief in an inset. The outline of these characteristics are:\n",
    "- familyName: Family name, the last name of a person. \n",
    "- givenName: Given name, the first name of a person. \n",
    "- alternateName: An alias for the chef. \n",
    "- birthDate: Date of birth, should be provided in the following ISO 8601 format: 'YYYY-MM-DD'. \n",
    "- gender: Gender of something, typically a Person, but possibly also fictional characters, animals, etc\n",
    "- homeLocation: An indication of a person' location\n",
    "- description: A short bio of the chef\n",
    "- image: should be the alternative description for the chef image.\n",
    "- prefCuisine: Preferred cuisine types (for example, French, Vietnamian, Bistronomy, Street-Food)\n",
    "\n",
    "Invent a chief, as a fictional but realistic character. \n",
    "In this chat, you may be asked to invent different chiefs, they shall be as diverse as possible and from all social backgrounds. \n",
    "\"\"\"\n",
    "\n",
    "helper.save_prompt(directory_prompt=DIR_PROMPTS, prompt=current_chefprompt, prompt_name=current_chefprompt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_recipeschema=Recipe\n",
    "current_recipeprompt_name=\"Recipe-b200p\"\n",
    "current_recipeprompt = \"\"\"\n",
    "You are a one of the chefs showcased in a famous international restaurant and cooking guide, open to all audiences, from the most modest to the most gastronomic. Here are some of your traits:\n",
    "- name: {givenName} {familyName}. \n",
    "- birthDate: {birthDate}\n",
    "- gender: {gender}\n",
    "- homeLocation: {homeLocation}\n",
    "- description: {description}\n",
    "- preferred cuisine types: {prefCuisine}\n",
    "\n",
    "You want to share a dish that is meaningful to you. You indicate the characteristics of the recipes, the ingredients and the steps in an orderly fashion:\n",
    "- id_chef_creator: shall be your '{givenName} {familyName}'\n",
    "- name: The name of the dish\n",
    "- recipeIngredient: The necessary ingredients and their quantities used in the recipe.\n",
    "- recipeYield: The quantity produced by the recipe, if applicable. Specify the number of servings produced from this recipe with just a number.\n",
    "- recipeInstructions: The steps to make the dish.\n",
    "- prepTime: The length of time it takes to prepare ingredients and workspace for the dish\n",
    "- cookTime: The time it takes to actually cook the dish\n",
    "- description: A short summary describing the dish.\n",
    "- image: should be the alternative description for the dish image.\n",
    "- recipeCategory: The type of meal or course your recipe is about. For example: \"dinner\", \"main course\", or \"dessert, snack\".\n",
    "- recipeCuisine: The region associated with your recipe. For example, \"French\", Mediterranean\", or \"American\".\n",
    "- keywords: Other terms for your recipe such as the season (\"summer\"), the holiday (\"Halloween\"), or other descriptors (\"quick\", \"easy\", \"authentic\").\n",
    "\n",
    "Invent a recipe, {givenName} {familyName}. In this chat, you may be asked to invent different recipes, they shall be as diverse as possible, include main dishes, snacks, appetizers, drinks, deserts, etc... \n",
    "\"\"\"\n",
    "# helper.save_prompt(directory_prompt=DIR_PROMPTS, prompt=current_recipeprompt, prompt_name=current_recipeprompt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = client.chat(\n",
    "#   messages=[\n",
    "#     {\n",
    "#       'role': 'user',\n",
    "#       'content': current_chefprompt,\n",
    "#     }\n",
    "#   ],\n",
    "#   model=LLAMA_MODEL,\n",
    "#   format=current_chefschema.model_json_schema(),\n",
    "# )\n",
    "# display(response)\n",
    "\n",
    "# out = current_chefschema.model_validate_json(response.message.content)\n",
    "# pprint(out)\n",
    "\n",
    "# current_recipeprompt.format(**out.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate\n",
    "*--- WIP ---*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *n* chefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.connection:connect_tcp.started host='localhost' port=11434 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E677294040>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Thu, 16 Jan 2025 21:57:03 GMT'), (b'Content-Length', b'1072')])\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "INFO:root:Saving new chef Pérez Ana with cuisines ['Mexican', 'Indigenous', 'Molecular Gastronomy']\n",
      "INFO:helper_io:Saving output c:\\_Etudes\\25_Main_Patte\\lamachef\\out\\250116_225703-Chef-b101-llama3.2.json\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Thu, 16 Jan 2025 21:57:13 GMT'), (b'Content-Length', b'1131')])\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "INFO:root:Saving new chef Kumar Rahul with cuisines ['Indian', 'Street Food']\n",
      "INFO:helper_io:Saving output c:\\_Etudes\\25_Main_Patte\\lamachef\\out\\250116_225713-Chef-b101-llama3.2.json\n"
     ]
    }
   ],
   "source": [
    "# for i in range(2):\n",
    "#     response = client.chat(\n",
    "#     messages=[\n",
    "#         {\n",
    "#         'role': 'user',\n",
    "#         'content': current_chefprompt,\n",
    "#         }\n",
    "#     ],\n",
    "#     model=LLAMA_MODEL,\n",
    "#     format=current_chefschema.model_json_schema(),\n",
    "#     )\n",
    "#     out_chef = current_chefschema.model_validate_json(response.message.content)\n",
    "#     logger.info(f\"Saving new chef {out_chef.familyName} {out_chef.givenName} with cuisines {out_chef.prefCuisine}\")\n",
    "#     helper.save_structured_output(output=out_chef.model_dump(), directory_outputs=DIR_OUTPUT, prompt_name=current_chefprompt_name, llm_model=LLAMA_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *n* chefs with *m* recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Saving new chef Pantillon Léon with cuisines ['French', 'Bistronomy']\n",
      "INFO:helper_io:Saving output c:\\_Etudes\\25_Main_Patte\\lamachef\\out\\250116_235540-Chef-b102-llama3.2.json\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Saving recipe Tournedos Rossini from chef Pantillon Léon as main course\n",
      "INFO:helper_io:Saving output c:\\_Etudes\\25_Main_Patte\\lamachef\\out\\250116_235624-Recipe-b200p-llama3.2.json\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Saving recipe Tournedos Rossini from chef Pantillon Léon as main course\n",
      "INFO:helper_io:Saving output c:\\_Etudes\\25_Main_Patte\\lamachef\\out\\250116_235651-Recipe-b200p-llama3.2.json\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Saving new chef Liu Mei with cuisines ['Chinese', 'Sichuan']\n",
      "INFO:helper_io:Saving output c:\\_Etudes\\25_Main_Patte\\lamachef\\out\\250116_235705-Chef-b102-llama3.2.json\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Saving recipe Double-Fried Pork Belly with Pickled Mustard Greens and Chili Oil from chef Liu Mei as Main Course\n",
      "INFO:helper_io:Saving output c:\\_Etudes\\25_Main_Patte\\lamachef\\out\\250116_235738-Recipe-b200p-llama3.2.json\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Saving recipe Spicy Sichuan Kung Pao Chicken with Wonton Noodles from chef Liu Mei as Main Course\n",
      "INFO:helper_io:Saving output c:\\_Etudes\\25_Main_Patte\\lamachef\\out\\250116_235807-Recipe-b200p-llama3.2.json\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    response = client.chat(\n",
    "    messages=[\n",
    "        {\n",
    "        'role': 'user',\n",
    "        'content': current_chefprompt,\n",
    "        }\n",
    "    ],\n",
    "    model=LLAMA_MODEL,\n",
    "    format=current_chefschema.model_json_schema(),\n",
    "    )\n",
    "    out_chef = current_chefschema.model_validate_json(response.message.content)\n",
    "    logger.info(f\"Saving new chef {out_chef.familyName} {out_chef.givenName} with cuisines {out_chef.prefCuisine}\")\n",
    "    helper.save_structured_output(output=out_chef.model_dump(), directory_outputs=DIR_OUTPUT, prompt_name=current_chefprompt_name, llm_model=LLAMA_MODEL)\n",
    "\n",
    "    for i in range(2):\n",
    "        response2 = client.chat(\n",
    "        messages=[\n",
    "            {\n",
    "            'role': 'user',\n",
    "            'content': current_recipeprompt.format(**out_chef.model_dump()),\n",
    "            }\n",
    "        ],\n",
    "        model=LLAMA_MODEL,\n",
    "        format=current_recipeschema.model_json_schema(),\n",
    "        )\n",
    "        out_recipe = current_recipeschema.model_validate_json(response2.message.content)\n",
    "        logger.info(f\"Saving recipe {out_recipe.name} from chef {out_chef.familyName} {out_chef.givenName} as {out_recipe.recipeCategory}\")\n",
    "        helper.save_structured_output(output=out_recipe.model_dump(), directory_outputs=DIR_OUTPUT, prompt_name=current_recipeprompt_name, llm_model=LLAMA_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
