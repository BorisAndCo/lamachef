{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *La Main à la Pâte*, Generate Chefs and Recipes\n",
    "*--- WIP ---*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from ollama import Client\n",
    "from pydantic import BaseModel\n",
    "from devtools import pprint\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.debug(\"\")\n",
    "\n",
    "from datetime import datetime\n",
    "import helper_io as helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='C:\\\\Users\\\\antgr\\\\anaconda3\\\\envs\\\\web310\\\\Library\\\\ssl\\\\cacert.pem'\n"
     ]
    }
   ],
   "source": [
    "# Conf. ollama and select model\n",
    "HOST_LLAMA=os.environ['HOST_LLAMA']\n",
    "LLAMA_MODEL=\"llama3.2\"\n",
    "\n",
    "client = Client(\n",
    "  host=HOST_LLAMA,\n",
    "  headers={'x-some-header': 'some-value'}\n",
    ")\n",
    "\n",
    "# Conf. io directories\n",
    "WORKDIR = Path(os.getcwd())\n",
    "DIR_SCHEMAS = Path(WORKDIR.parent, 'json_schemas')\n",
    "DIR_OUTPUT = Path(WORKDIR.parent, 'out')\n",
    "DIR_PROMPTS = Path(WORKDIR.parent, 'prompts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "To use Ollama's structured output, the LLM-model answer to the prompt is constrained with a *JSON Schema*, conveniently registered in Python with a Pydantic Model.\n",
    "The *JSON schemas* are registered in the `DIR_MODELS` directory. We provide ios functions for saving/loading schemas in `src/helper_io.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemples of models\n",
    "class Chef(BaseModel):\n",
    "    familyName:     str\n",
    "    givenName:      str\n",
    "    alternateName:  str\n",
    "    birthDate:      str # Problem with pydantic datetime declaration\n",
    "    gender:         str\n",
    "    description:    str\n",
    "    homeLocation:   str \n",
    "    image:          str \n",
    "    prefCuisine:    list[str] \n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    id_chef_creator:        str\n",
    "    name:                   str\n",
    "    recipeIngredients:      list[str]\n",
    "    recipeYield:            str\n",
    "    recipeInstructions:     list[str]\n",
    "    prepTime:               int\n",
    "    cookTime:               int\n",
    "    description:            str\n",
    "    image:                  str\n",
    "    recipeCategory:         str\n",
    "    recipeCuisine:          list[str]\n",
    "    keywords:               list[str]\n",
    "\n",
    "\n",
    "# # Save an existing model\n",
    "# helper.save_pydantic_model_as_jsonschema(directory_schema=DIR_SCHEMAS, schema_pydmodel=Chef)\n",
    "# helper.save_pydantic_model_as_jsonschema(directory_schema=DIR_SCHEMAS, schema_pydmodel=Recipe)\n",
    "\n",
    "# Load an existing model\n",
    "# Animal = helper.load_jsonschema_to_pydantic_model(json.loads(Path(WORKDIR.parent, 'json_schemas', 'Animal.json').read_text()))\n",
    "\n",
    "current_schema=Chef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts\n",
    "*--- WIP ---*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a prompt\n",
    "# helper.save_pydantic_model(directory_models: str|Path, model: BaseModel)\n",
    "\n",
    "# Load an existing prompt\n",
    "# helper.load_pydantic_model(directory_models: str|Path, model: BaseModel) -> BaseModel\n",
    "\n",
    "current_chefprompt_name=\"Chef-b101\"\n",
    "current_chefprompt = \"\"\"You are the editor-in-chief of a famous international restaurant and cooking guide, open to all audiences, from the most modest to the most gastronomic (for example traditional french cuisine, chinese street food or english bistronomy). You are known for picking out vivid personalities, eager to share their best recipes and opiniated tastes. \n",
    "You want to exhibit one of you favorite chief in the pages of your next issue. As a summary, you specify some key characteristics of this chief in an inset. The outline of these characteristics are:\n",
    "- familyName: Family name, the last name of a person. \n",
    "- givenName: Given name, the first name of a person. \n",
    "- alternateName: An alias for the chef. \n",
    "- birthDate: Date of birth, should be provided in the following ISO 8601 format: 'YYYY-MM-DD'. \n",
    "- gender: Gender of something, typically a Person, but possibly also fictional characters, animals, etc\n",
    "- homeLocation: An indication of a person' location\n",
    "- description: A short bio of the chef\n",
    "- image: should be the alternative description for the chef image.\n",
    "- prefCuisine: Preferred cuisine types (for example, French, Vietnamian, Bistronomy, Street-Food)\n",
    "\n",
    "Invent a chief, as a fictional but realistic character. \n",
    "In this chat, you may be asked to invent different chiefs, they shall be as diverse as possible to create a fresco of world cuisine from all social backgrounds. \n",
    "\"\"\"\n",
    "\n",
    "# helper.save_prompt(directory_prompt=DIR_PROMPTS, prompt=current_prompt, prompt_name=current_prompt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:helper_io:Saving prompt Recipe-b100 to c:\\_Etudes\\25_Main_Patte\\lamachef\\prompts\\Recipe-b100.md\n"
     ]
    }
   ],
   "source": [
    "current_recipeprompt_name=\"Recipe-b200p\"\n",
    "current_recipeprompt = \"\"\"\n",
    "You are a one of the chefs showcased in a famous international restaurant and cooking guide, open to all audiences, from the most modest to the most gastronomic. Here are some of your traits:\n",
    "- name: {givenName} {familyName}. \n",
    "- birthDate: Date of birth, should be provided in the following ISO 8601 format: 'YYYY-MM-DD'. \n",
    "- gender: {gender}\n",
    "- homeLocation: {homeLocation}\n",
    "- description: {description}\n",
    "- preferred cuisine types: {prefCuisine}\n",
    "\n",
    "\n",
    "You want to share a dish that is meaningful to you. You indicate the characteristics of the recipes, the ingredients and the steps in an orderly fashion:\n",
    "- id_chef_creator: shall be your '{givenName} {familyName}'\n",
    "- name: The name of the dish\n",
    "- recipeIngredient: The necessary ingredients and their quantities used in the recipe.\n",
    "- recipeYield: The quantity produced by the recipe, if applicable. Specify the number of servings produced from this recipe with just a number.\n",
    "- recipeInstructions: The steps to make the dish.\n",
    "- prepTime: The length of time it takes to prepare ingredients and workspace for the dish\n",
    "- cookTime: The time it takes to actually cook the dish\n",
    "- description: A short summary describing the dish.\n",
    "- image: should be the alternative description for the dish image.\n",
    "- recipeCategory: The type of meal or course your recipe is about. For example: \"dinner\", \"main course\", or \"dessert, snack\".\n",
    "- recipeCuisine: The region associated with your recipe. For example, \"French\", Mediterranean\", or \"American\".\n",
    "- keywords: Other terms for your recipe such as the season (\"summer\"), the holiday (\"Halloween\"), or other descriptors (\"quick\", \"easy\", \"authentic\").\n",
    "\n",
    "\n",
    "Invent a recipe of {givenName} {familyName}. In this chat, you may be asked to invent different recipes\n",
    "\n",
    "\n",
    "different chiefs, they shall be as diverse as possible to create a fresco of world cuisine from all social backgrounds. \n",
    "\n",
    "Invent the 6 key recipes of Jean-Pierre Bourgeois. The choice of these 6 recipes shall include at least one appetizer, one main dish, one desert and one drink. This choice shall be motivated by a short description, in the motivation field of groupRecipe. Your favorite - and most exortic - ingredient shall be featured in at least one of those recipes, and motivated in the secretIntegredient field of groupRecipe.\n",
    "\n",
    "\"\"\"\n",
    "helper.save_prompt(directory_prompt=DIR_PROMPTS, prompt=current_recipeprompt, prompt_name=current_recipeprompt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = client.chat(\n",
    "#   messages=[\n",
    "#     {\n",
    "#       'role': 'user',\n",
    "#       'content': current_prompt,\n",
    "#     }\n",
    "#   ],\n",
    "#   model=LLAMA_MODEL,\n",
    "#   format=current_schema.model_json_schema(),\n",
    "# )\n",
    "# display(response)\n",
    "\n",
    "# out = current_schema.model_validate_json(response.message.content)\n",
    "# pprint(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate n chefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.connection:connect_tcp.started host='localhost' port=11434 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E677294040>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Thu, 16 Jan 2025 21:57:03 GMT'), (b'Content-Length', b'1072')])\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "INFO:root:Saving new chef Pérez Ana with cuisines ['Mexican', 'Indigenous', 'Molecular Gastronomy']\n",
      "INFO:helper_io:Saving output c:\\_Etudes\\25_Main_Patte\\lamachef\\out\\250116_225703-Chef-b101-llama3.2.json\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Thu, 16 Jan 2025 21:57:13 GMT'), (b'Content-Length', b'1131')])\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "INFO:root:Saving new chef Kumar Rahul with cuisines ['Indian', 'Street Food']\n",
      "INFO:helper_io:Saving output c:\\_Etudes\\25_Main_Patte\\lamachef\\out\\250116_225713-Chef-b101-llama3.2.json\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    response = client.chat(\n",
    "    messages=[\n",
    "        {\n",
    "        'role': 'user',\n",
    "        'content': current_chefprompt,\n",
    "        }\n",
    "    ],\n",
    "    model=LLAMA_MODEL,\n",
    "    format=current_schema.model_json_schema(),\n",
    "    )\n",
    "    out = current_schema.model_validate_json(response.message.content)\n",
    "    logger.info(f\"Saving new chef {out.familyName} {out.givenName} with cuisines {out.prefCuisine}\")\n",
    "    helper.save_structured_output(output=out.model_dump(), directory_outputs=DIR_OUTPUT, prompt_name=current_chefprompt_name, llm_model=LLAMA_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
