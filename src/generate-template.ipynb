{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template Notebook for LLM Structured Outputs\n",
    "*--- WIP ---*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from ollama import Client\n",
    "from pydantic import BaseModel\n",
    "from devtools import pprint\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.debug(\"\")\n",
    "\n",
    "from datetime import datetime\n",
    "import helper_io as helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conf. ollama and select model\n",
    "HOST_LLAMA=os.environ['HOST_LLAMA']\n",
    "LLAMA_MODEL=\"llama3.2\"\n",
    "\n",
    "client = Client(\n",
    "  host=HOST_LLAMA,\n",
    "  headers={'x-some-header': 'some-value'}\n",
    ")\n",
    "\n",
    "# Conf. io directories\n",
    "WORKDIR = Path(os.getcwd())\n",
    "DIR_SCHEMAS = Path(WORKDIR.parent, 'json_schemas')\n",
    "DIR_OUTPUT = Path(WORKDIR.parent, 'out')\n",
    "DIR_PROMPTS = Path(WORKDIR.parent, 'prompts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "To use Ollama's structured output, the LLM-model answer to the prompt is constrained with a *JSON Schema*, conveniently registered in Python with a Pydantic Model.\n",
    "The *JSON schemas* are registered in the `DIR_MODELS` directory. We provide ios functions for saving/loading schemas in `src/helper_io.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemples of models\n",
    "class Chef(BaseModel):\n",
    "    familyName:     str\n",
    "    givenName:      str\n",
    "    alternateName:  str\n",
    "    # birthDate:      datetime.date\n",
    "    gender:         str\n",
    "    description:    str\n",
    "    homeLocation:   str \n",
    "    image:          str \n",
    "    prefCuisine:    list[str] \n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    name:                   str\n",
    "    recipeIngredients:      list[str]\n",
    "    recipeYield:            str\n",
    "    recipeInstructions:     list[str]\n",
    "    prepTime:               int\n",
    "    cookTime:               int\n",
    "    description:            str\n",
    "    image:                  str\n",
    "    recipeCategory:         str\n",
    "    recipeCuisine:          list[str]\n",
    "    keywords:               list[str]\n",
    "\n",
    "\n",
    "# # Save an existing model\n",
    "# helper.save_pydantic_model_as_jsonschema(directory_schema=DIR_SCHEMAS, schema_pydmodel=Animal)\n",
    "\n",
    "# Load an existing model\n",
    "Animal = helper.load_jsonschema_to_pydantic_model(json.loads(Path(WORKDIR.parent, 'json_schemas', 'Animal.json').read_text()))\n",
    "current_schema=Animal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts\n",
    "*--- WIP ---*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a prompt\n",
    "# helper.save_pydantic_model(directory_models: str|Path, model: BaseModel)\n",
    "\n",
    "# Load an existing prompt\n",
    "# helper.load_pydantic_model(directory_models: str|Path, model: BaseModel) -> BaseModel\n",
    "\n",
    "current_prompt_name=\"Animal-b100\"\n",
    "current_prompt = \"\"\"You are a famous zoologist, known for having travelled all over the world, discovering new species and documenting them with a systematic method, and sensitivy. Write the identification card of the new animal you just discovered, following the new traits:\n",
    "+ name: the name of the animal\n",
    "+ binomial_name: the scientific name of the animal\n",
    "+ conservation_status: The conservation status of a group of organisms (for instance, a species) indicates whether the group still exists and how likely the group is to become (Not Evaluated, Data Deficient, Least Concern, Near Threatened, Vulnerable, Endangered, Critically Endangered, Extinct) \n",
    "+ description: a short description\n",
    "\"\"\"\n",
    "\n",
    "# helper.save_prompt(directory_prompt=DIR_PROMPTS, prompt=current_prompt, prompt_name=current_prompt_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send\n",
    "*--- WIP ---*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat(\n",
    "  messages=[\n",
    "    {\n",
    "      'role': 'user',\n",
    "      'content': current_prompt,\n",
    "    }\n",
    "  ],\n",
    "  model=LLAMA_MODEL,\n",
    "  format=current_schema.model_json_schema(),\n",
    ")\n",
    "display(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = current_schema.model_validate_json(response.message.content)\n",
    "pprint(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_structured_output(output=out.model_dump(), directory_outputs=DIR_OUTPUT, prompt_name=current_prompt_name, llm_model=LLAMA_MODEL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
